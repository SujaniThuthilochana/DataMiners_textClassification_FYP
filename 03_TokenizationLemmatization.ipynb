{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1d58f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "94cb8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f=pd.read_csv(\"pre_processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "13500e25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>Content_Parsed_0</th>\n",
       "      <th>Content_Parsed_1</th>\n",
       "      <th>Content_Parsed_2</th>\n",
       "      <th>Content_Parsed_3</th>\n",
       "      <th>Content_Parsed_4</th>\n",
       "      <th>Content_Parsed_5</th>\n",
       "      <th>Content_Parsed_6</th>\n",
       "      <th>Content_Parsed_7</th>\n",
       "      <th>Content_Parsed_8</th>\n",
       "      <th>Content_Parsed_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142536</th>\n",
       "      <td>161558</td>\n",
       "      <td>Mon Jan 24 00:59:53 +0000 2022</td>\n",
       "      <td>en</td>\n",
       "      <td>@MichaelFieldNZ Maybe a fav of the older gener...</td>\n",
       "      <td>@MichaelFieldNZ Maybe a fav of the older gener...</td>\n",
       "      <td>Maybe a fav of the older generation perhaps (o...</td>\n",
       "      <td>Maybe a fav of the older generation perhaps (o...</td>\n",
       "      <td>Maybe a fav of the older generation perhaps (o...</td>\n",
       "      <td>Maybe a fav of the older generation perhaps (o...</td>\n",
       "      <td>maybe a fav of the older generation perhaps (o...</td>\n",
       "      <td>maybe a fav of the older generation perhaps  o...</td>\n",
       "      <td>maybe a fav of the older generation perhaps  o...</td>\n",
       "      <td>maybe fav of the older generation perhaps our ...</td>\n",
       "      <td>maybe fav of the older generation perhaps our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142537</th>\n",
       "      <td>161559</td>\n",
       "      <td>Mon Jan 24 00:59:54 +0000 2022</td>\n",
       "      <td>en</td>\n",
       "      <td>🚨Very Bad Idea!🚨\\nPutting hospitalized #Covid1...</td>\n",
       "      <td>🚨Very Bad Idea!🚨\\nPutting hospitalized #Covid1...</td>\n",
       "      <td>🚨Very Bad Idea!🚨\\nPutting hospitalized #Covid1...</td>\n",
       "      <td>🚨Very Bad Idea!🚨\\nPutting hospitalized #Covid1...</td>\n",
       "      <td>🚨Very Bad Idea!🚨Putting hospitalized #Covid19 ...</td>\n",
       "      <td>🚨Very Bad Idea!🚨Putting hospitalized Covid19 p...</td>\n",
       "      <td>🚨very bad idea!🚨putting hospitalized covid19 p...</td>\n",
       "      <td>🚨very bad idea 🚨putting hospitalized covid19 p...</td>\n",
       "      <td>🚨very bad idea 🚨putting hospitalized covid pos...</td>\n",
       "      <td>🚨very bad idea 🚨putting hospitalized covid pos...</td>\n",
       "      <td>🚨very bad idea 🚨putting hospitalized covid pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142538</th>\n",
       "      <td>161560</td>\n",
       "      <td>Mon Jan 24 00:59:54 +0000 2022</td>\n",
       "      <td>en</td>\n",
       "      <td>The truckers do realize that even if the vacci...</td>\n",
       "      <td>The truckers do realize that even if the vacci...</td>\n",
       "      <td>The truckers do realize that even if the vacci...</td>\n",
       "      <td>The truckers do realize that even if the vacci...</td>\n",
       "      <td>The truckers do realize that even if the vacci...</td>\n",
       "      <td>The truckers do realize that even if the vacci...</td>\n",
       "      <td>the truckers do realize that even if the vacci...</td>\n",
       "      <td>the truckers do realize that even if the vacci...</td>\n",
       "      <td>the truckers do realize that even if the vacci...</td>\n",
       "      <td>the truckers do realize that even if the vacci...</td>\n",
       "      <td>the truckers do realize that even if the vacci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142539</th>\n",
       "      <td>161561</td>\n",
       "      <td>Mon Jan 24 00:59:53 +0000 2022</td>\n",
       "      <td>en</td>\n",
       "      <td>@oneunderscore__ Exploiting of the tragedy of ...</td>\n",
       "      <td>@oneunderscore__ Exploiting of the tragedy of ...</td>\n",
       "      <td>Exploiting of the tragedy of people who suffer...</td>\n",
       "      <td>Exploiting of the tragedy of people who suffer...</td>\n",
       "      <td>Exploiting of the tragedy of people who suffer...</td>\n",
       "      <td>Exploiting of the tragedy of people who suffer...</td>\n",
       "      <td>exploiting of the tragedy of people who suffer...</td>\n",
       "      <td>exploiting of the tragedy of people who suffer...</td>\n",
       "      <td>exploiting of the tragedy of people who suffer...</td>\n",
       "      <td>exploiting of the tragedy of people who suffer...</td>\n",
       "      <td>exploiting of the tragedy of people who suffer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142540</th>\n",
       "      <td>161562</td>\n",
       "      <td>Mon Jan 24 00:59:54 +0000 2022</td>\n",
       "      <td>en</td>\n",
       "      <td>K'taka reported 50k mark covid cases, with 50,...</td>\n",
       "      <td>K'taka reported 50k mark covid cases, with 50,...</td>\n",
       "      <td>K'taka reported 50k mark covid cases, with 50,...</td>\n",
       "      <td>K'taka reported 50k mark covid cases, with 50,...</td>\n",
       "      <td>K'taka reported 50k mark covid cases, with 50,...</td>\n",
       "      <td>K'taka reported 50k mark covid cases, with 50,...</td>\n",
       "      <td>k'taka reported 50k mark covid cases, with 50,...</td>\n",
       "      <td>k taka reported 50k mark covid cases  with 50 ...</td>\n",
       "      <td>k taka reported k mark covid cases  with   in ...</td>\n",
       "      <td>taka reported mark covid cases with in number...</td>\n",
       "      <td>taka reported mark covid cases with in number...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                      created_at lang  \\\n",
       "142536      161558  Mon Jan 24 00:59:53 +0000 2022   en   \n",
       "142537      161559  Mon Jan 24 00:59:54 +0000 2022   en   \n",
       "142538      161560  Mon Jan 24 00:59:54 +0000 2022   en   \n",
       "142539      161561  Mon Jan 24 00:59:53 +0000 2022   en   \n",
       "142540      161562  Mon Jan 24 00:59:54 +0000 2022   en   \n",
       "\n",
       "                                                     text  \\\n",
       "142536  @MichaelFieldNZ Maybe a fav of the older gener...   \n",
       "142537  🚨Very Bad Idea!🚨\\nPutting hospitalized #Covid1...   \n",
       "142538  The truckers do realize that even if the vacci...   \n",
       "142539  @oneunderscore__ Exploiting of the tragedy of ...   \n",
       "142540  K'taka reported 50k mark covid cases, with 50,...   \n",
       "\n",
       "                                         Content_Parsed_0  \\\n",
       "142536  @MichaelFieldNZ Maybe a fav of the older gener...   \n",
       "142537  🚨Very Bad Idea!🚨\\nPutting hospitalized #Covid1...   \n",
       "142538  The truckers do realize that even if the vacci...   \n",
       "142539  @oneunderscore__ Exploiting of the tragedy of ...   \n",
       "142540  K'taka reported 50k mark covid cases, with 50,...   \n",
       "\n",
       "                                         Content_Parsed_1  \\\n",
       "142536  Maybe a fav of the older generation perhaps (o...   \n",
       "142537  🚨Very Bad Idea!🚨\\nPutting hospitalized #Covid1...   \n",
       "142538  The truckers do realize that even if the vacci...   \n",
       "142539  Exploiting of the tragedy of people who suffer...   \n",
       "142540  K'taka reported 50k mark covid cases, with 50,...   \n",
       "\n",
       "                                         Content_Parsed_2  \\\n",
       "142536  Maybe a fav of the older generation perhaps (o...   \n",
       "142537  🚨Very Bad Idea!🚨\\nPutting hospitalized #Covid1...   \n",
       "142538  The truckers do realize that even if the vacci...   \n",
       "142539  Exploiting of the tragedy of people who suffer...   \n",
       "142540  K'taka reported 50k mark covid cases, with 50,...   \n",
       "\n",
       "                                         Content_Parsed_3  \\\n",
       "142536  Maybe a fav of the older generation perhaps (o...   \n",
       "142537  🚨Very Bad Idea!🚨Putting hospitalized #Covid19 ...   \n",
       "142538  The truckers do realize that even if the vacci...   \n",
       "142539  Exploiting of the tragedy of people who suffer...   \n",
       "142540  K'taka reported 50k mark covid cases, with 50,...   \n",
       "\n",
       "                                         Content_Parsed_4  \\\n",
       "142536  Maybe a fav of the older generation perhaps (o...   \n",
       "142537  🚨Very Bad Idea!🚨Putting hospitalized Covid19 p...   \n",
       "142538  The truckers do realize that even if the vacci...   \n",
       "142539  Exploiting of the tragedy of people who suffer...   \n",
       "142540  K'taka reported 50k mark covid cases, with 50,...   \n",
       "\n",
       "                                         Content_Parsed_5  \\\n",
       "142536  maybe a fav of the older generation perhaps (o...   \n",
       "142537  🚨very bad idea!🚨putting hospitalized covid19 p...   \n",
       "142538  the truckers do realize that even if the vacci...   \n",
       "142539  exploiting of the tragedy of people who suffer...   \n",
       "142540  k'taka reported 50k mark covid cases, with 50,...   \n",
       "\n",
       "                                         Content_Parsed_6  \\\n",
       "142536  maybe a fav of the older generation perhaps  o...   \n",
       "142537  🚨very bad idea 🚨putting hospitalized covid19 p...   \n",
       "142538  the truckers do realize that even if the vacci...   \n",
       "142539  exploiting of the tragedy of people who suffer...   \n",
       "142540  k taka reported 50k mark covid cases  with 50 ...   \n",
       "\n",
       "                                         Content_Parsed_7  \\\n",
       "142536  maybe a fav of the older generation perhaps  o...   \n",
       "142537  🚨very bad idea 🚨putting hospitalized covid pos...   \n",
       "142538  the truckers do realize that even if the vacci...   \n",
       "142539  exploiting of the tragedy of people who suffer...   \n",
       "142540  k taka reported k mark covid cases  with   in ...   \n",
       "\n",
       "                                         Content_Parsed_8  \\\n",
       "142536  maybe fav of the older generation perhaps our ...   \n",
       "142537  🚨very bad idea 🚨putting hospitalized covid pos...   \n",
       "142538  the truckers do realize that even if the vacci...   \n",
       "142539  exploiting of the tragedy of people who suffer...   \n",
       "142540   taka reported mark covid cases with in number...   \n",
       "\n",
       "                                         Content_Parsed_9  \n",
       "142536  maybe fav of the older generation perhaps our ...  \n",
       "142537  🚨very bad idea 🚨putting hospitalized covid pos...  \n",
       "142538  the truckers do realize that even if the vacci...  \n",
       "142539  exploiting of the tragedy of people who suffer...  \n",
       "142540   taka reported mark covid cases with in number...  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_f.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9ff6d032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remember what jared kushner dropped national covid strategy because he didn give ck about blue states hope they haul his ass in too '"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_f.loc[132266]['Content_Parsed_9']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd12697",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d503d39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "952cc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f['Content_Parsed_9'] = new_f['Content_Parsed_9'].apply(str)\n",
    "\n",
    "# Tokenize the text in the 'text' column\n",
    "new_f['Content_Parsed_10'] = new_f['Content_Parsed_9'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e3b49a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trumps',\n",
       " 'impeachment',\n",
       " 'distracted',\n",
       " 'him',\n",
       " 'from',\n",
       " 'preparing',\n",
       " 'for',\n",
       " 'pandemic',\n",
       " 'but',\n",
       " 'the',\n",
       " 'pandemic',\n",
       " 'did',\n",
       " 'not',\n",
       " 'distract',\n",
       " 'him',\n",
       " 'from',\n",
       " 'firing',\n",
       " 'the',\n",
       " 'man',\n",
       " 'he',\n",
       " 'holds',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'his',\n",
       " 'impeachment']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_f.loc[2]['Content_Parsed_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "25f3831f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mksuj\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mksuj\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mksuj\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\mksuj\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\mksuj\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mksuj\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Collecting wordnet\n",
      "  Downloading wordnet-0.0.1b2.tar.gz (8.8 kB)\n",
      "Collecting colorama==0.3.9\n",
      "  Downloading colorama-0.3.9-py2.py3-none-any.whl (20 kB)\n",
      "Building wheels for collected packages: wordnet\n",
      "  Building wheel for wordnet (setup.py): started\n",
      "  Building wheel for wordnet (setup.py): finished with status 'done'\n",
      "  Created wheel for wordnet: filename=wordnet-0.0.1b2-py3-none-any.whl size=10521 sha256=a518ac1915e701fb88dc01efe4ff9542e379b572762b58dd5f32b20f1c5bfe84\n",
      "  Stored in directory: c:\\users\\mksuj\\appdata\\local\\pip\\cache\\wheels\\01\\be\\ef\\2424637bbd3690b10b7fc1c16faa12fdd053ff6b1abb2c3c08\n",
      "Successfully built wordnet\n",
      "Installing collected packages: colorama, wordnet\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "Successfully installed colorama-0.3.9 wordnet-0.0.1b2\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install wordnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd54ef",
   "metadata": {},
   "source": [
    "POS Tagging and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "9c65447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3b539195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mksuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a94649f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun\n",
    "\n",
    "nrows = len(new_f)\n",
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    tokens = new_f.loc[row]['Content_Parsed_10']\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for token, pos in pos_tags:\n",
    "        # Determine the part of speech of the token\n",
    "        pos = get_wordnet_pos(pos)\n",
    "        # Lemmatize the token\n",
    "        lemma = lemmatizer.lemmatize(token, pos)\n",
    "        lemmas.append(lemma)\n",
    "        \n",
    "    lemmatized_text = \" \".join(lemmas)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b86fb09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f['Content_Parsed_11'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "89e803e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'john lewis will pay full sick pay to staff if they have to isolate for covid whether they re jabbed or not thank you for do the right thing 👏👏👏👏and shame on the other retailer who seek to divide u'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_f.loc[79074]['Content_Parsed_11']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd13fbb",
   "metadata": {},
   "source": [
    "#Stop words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "bd70d532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mksuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ca03c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "dd730ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words that are in NLTK stopwords list\n",
    "not_stopwords = {'not'} \n",
    "updated_stopwords = set([word for word in stop_words if word not in not_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "63b384e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(new_f)\n",
    "filtered_words = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    s_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = new_f.loc[row]['Content_Parsed_11']\n",
    "    text_words = str(text).split(\" \")\n",
    "    for word in text_words:\n",
    "    # If the word is not a stop word, add it to the filtered list\n",
    "        if word not in updated_stopwords:\n",
    "            s_list.append(word)\n",
    "        \n",
    "    # Join the list\n",
    "    stop_text = \" \".join(s_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    filtered_words.append(stop_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "22406afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f['Content_Parsed_12'] = filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d1642",
   "metadata": {},
   "source": [
    "#consecutive emoji seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "732bc8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(new_f)\n",
    "text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = new_f.loc[row]['Content_Parsed_12']\n",
    "    words_and_emojis = re.split(r'([\\U0001f600-\\U0001f64f\\U0001f300-\\U0001f5ff\\U0001f680-\\U0001f6ff\\U0001f1e0-\\U0001f1ff\\U00002702-\\U000027b0])', str(text))\n",
    "    \n",
    "    elements = []\n",
    "    for element in words_and_emojis:\n",
    "        if re.search(r'([\\U0001f600-\\U0001f64f\\U0001f300-\\U0001f5ff\\U0001f680-\\U0001f6ff\\U0001f1e0-\\U0001f1ff\\U00002702-\\U000027b0])', element):\n",
    "            for emoji in element:\n",
    "                elements.append(emoji)  \n",
    "        else:\n",
    "                elements.append(element)\n",
    "    \n",
    "    emo_text = \" \".join(elements)\n",
    "    text_list.append(emo_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a62d9cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f['Content_Parsed_13'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "2a3631ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(new_f)\n",
    "text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    text = new_f.loc[row]['Content_Parsed_13']\n",
    "    new_list = text.split()\n",
    "    \n",
    "    text_list.append(new_list)\n",
    "    \n",
    "new_f['Content_Parsed_13'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "88de58e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['african',\n",
       " 'country',\n",
       " 'poor',\n",
       " 'healthcare',\n",
       " 'sanitation',\n",
       " 'covid',\n",
       " 'kill',\n",
       " 'many',\n",
       " 'people',\n",
       " 'overwhelm',\n",
       " 'hospital',\n",
       " 'not',\n",
       " 'sick',\n",
       " 'people',\n",
       " 'get',\n",
       " 'treatment',\n",
       " 'outbreak',\n",
       " 'africa',\n",
       " 'magnitude',\n",
       " 'one',\n",
       " 'u',\n",
       " 'europe',\n",
       " 'spain',\n",
       " 'italy',\n",
       " 'esp',\n",
       " 'would']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_f.loc[4]['Content_Parsed_13']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d67032",
   "metadata": {},
   "source": [
    "#Remove duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "af4d1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def remove_duplicate_words(lemmas):\n",
    "  # Create a new list of unique lemmas\n",
    "  unique_lemmas = []\n",
    "  for lemma in lemmas:\n",
    "    if lemma not in unique_lemmas:\n",
    "      unique_lemmas.append(lemma)\n",
    "  return unique_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1cb90d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(new_f)\n",
    "text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    word_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = new_f.loc[row]['Content_Parsed_13']\n",
    "    text_words = str(text).split(\" \")\n",
    "    \n",
    "    list_text = \" \".join(text_words)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    text_list.append(list_text)\n",
    "    \n",
    "for i, element in enumerate(text_list):\n",
    "    # Apply ast.literal_eval() to the element\n",
    "    element = ast.literal_eval(element)\n",
    "    # Replace the element in the list with the evaluated value\n",
    "    text_list[i] = element\n",
    "    \n",
    "new_f['Content_Parsed_14'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "8a7c769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f['Content_Parsed_14'] = new_f['Content_Parsed_14'].apply(remove_duplicate_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c244049c",
   "metadata": {},
   "source": [
    "#Remove Emojis and Replace it with its meaning as a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "9444a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9757a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = str(text).replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7e01e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(new_f)\n",
    "emoji_text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    emo_list = []\n",
    "    text1 = new_f.iloc[row]['Content_Parsed_14']\n",
    "    textEmo = convert_emojis(text1)\n",
    "    emoji_text_list.append(textEmo)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "db6f5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f['Content_Parsed_15'] = emoji_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7e52b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "f = lambda x: \" \".join(w for w in nltk.wordpunct_tokenize(x) if w.lower() in words)\n",
    "new_f['Content_Parsed_16'] = new_f['Content_Parsed_15'].apply(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c2ac38",
   "metadata": {},
   "source": [
    "Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "84062180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'john lewis will pay full sick pay to staff if they have to isolate for covid whether they re jabbed or not thank you for doing the right thing 👏👏👏👏and shame on the other retailers who seek to divide us '"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaned text\n",
    "new_f.loc[79074]['Content_Parsed_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d0b17d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'lewis',\n",
       " 'will',\n",
       " 'pay',\n",
       " 'full',\n",
       " 'sick',\n",
       " 'pay',\n",
       " 'to',\n",
       " 'staff',\n",
       " 'if',\n",
       " 'they',\n",
       " 'have',\n",
       " 'to',\n",
       " 'isolate',\n",
       " 'for',\n",
       " 'covid',\n",
       " 'whether',\n",
       " 'they',\n",
       " 're',\n",
       " 'jabbed',\n",
       " 'or',\n",
       " 'not',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'for',\n",
       " 'doing',\n",
       " 'the',\n",
       " 'right',\n",
       " 'thing',\n",
       " '👏👏👏👏and',\n",
       " 'shame',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'retailers',\n",
       " 'who',\n",
       " 'seek',\n",
       " 'to',\n",
       " 'divide',\n",
       " 'us']"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "new_f.loc[79074]['Content_Parsed_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "06811ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'john lewis will pay full sick pay to staff if they have to isolate for covid whether they re jabbed or not thank you for do the right thing 👏👏👏👏and shame on the other retailer who seek to divide u'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS and Lemmatization\n",
    "new_f.loc[79074]['Content_Parsed_11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "c4d68d74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'john lewis pay full sick pay staff isolate covid whether jabbed not thank right thing 👏👏👏👏and shame retailer seek divide u'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stop word\n",
    "new_f.loc[79074]['Content_Parsed_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "b974de70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'lewis',\n",
       " 'pay',\n",
       " 'full',\n",
       " 'sick',\n",
       " 'pay',\n",
       " 'staff',\n",
       " 'isolate',\n",
       " 'covid',\n",
       " 'whether',\n",
       " 'jabbed',\n",
       " 'not',\n",
       " 'thank',\n",
       " 'right',\n",
       " 'thing',\n",
       " '👏',\n",
       " '👏',\n",
       " '👏',\n",
       " '👏',\n",
       " 'and',\n",
       " 'shame',\n",
       " 'retailer',\n",
       " 'seek',\n",
       " 'divide',\n",
       " 'u']"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seperate emojies\n",
    "new_f.loc[79074]['Content_Parsed_13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e61bec3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'lewis',\n",
       " 'pay',\n",
       " 'full',\n",
       " 'sick',\n",
       " 'staff',\n",
       " 'isolate',\n",
       " 'covid',\n",
       " 'whether',\n",
       " 'jabbed',\n",
       " 'not',\n",
       " 'thank',\n",
       " 'right',\n",
       " 'thing',\n",
       " '👏',\n",
       " 'and',\n",
       " 'shame',\n",
       " 'retailer',\n",
       " 'seek',\n",
       " 'divide',\n",
       " 'u']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove duplications\n",
    "new_f.loc[79074]['Content_Parsed_14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2518012e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['john', 'lewis', 'pay', 'full', 'sick', 'staff', 'isolate', 'covid', 'whether', 'jabbed', 'not', 'thank', 'right', 'thing', 'clapping_hands', 'and', 'shame', 'retailer', 'seek', 'divide', 'u']\""
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace emoji with word\n",
    "new_f.loc[79074]['Content_Parsed_15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "33abd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_columns =[\"created_at\",\"text\",\"Content_Parsed_15\"]\n",
    "new_f=new_f[main_columns]\n",
    "\n",
    "new_f = new_f.rename(columns={'created_at':'Date','text':'Orginal tweet','Content_Parsed_15':'Preprocessed tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "03e2d98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Orginal tweet</th>\n",
       "      <th>Preprocessed tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat Apr 04 12:04:28 +0000 2020</td>\n",
       "      <td>I'm still learning how to survive on my own. T...</td>\n",
       "      <td>['still', 'learn', 'survive', 'pandemic', 'pus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sat Apr 04 12:04:28 +0000 2020</td>\n",
       "      <td>We all need to fight #COVID19.\\nAnyone can get...</td>\n",
       "      <td>['need', 'fight', 'covid', 'anyone', 'get', 'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sat Apr 04 12:04:28 +0000 2020</td>\n",
       "      <td>Trump’s impeachment distracted him from prepar...</td>\n",
       "      <td>['trump', 'impeachment', 'distract', 'prepare'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sat Apr 04 12:04:28 +0000 2020</td>\n",
       "      <td>@PIB_India @prasoonjoshi_ We will beat the corona</td>\n",
       "      <td>['beat', 'corona']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat Apr 04 12:04:28 +0000 2020</td>\n",
       "      <td>2. Most African countries have poor healthcare...</td>\n",
       "      <td>['african', 'country', 'poor', 'healthcare', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Date  \\\n",
       "0  Sat Apr 04 12:04:28 +0000 2020   \n",
       "1  Sat Apr 04 12:04:28 +0000 2020   \n",
       "2  Sat Apr 04 12:04:28 +0000 2020   \n",
       "3  Sat Apr 04 12:04:28 +0000 2020   \n",
       "4  Sat Apr 04 12:04:28 +0000 2020   \n",
       "\n",
       "                                       Orginal tweet  \\\n",
       "0  I'm still learning how to survive on my own. T...   \n",
       "1  We all need to fight #COVID19.\\nAnyone can get...   \n",
       "2  Trump’s impeachment distracted him from prepar...   \n",
       "3  @PIB_India @prasoonjoshi_ We will beat the corona   \n",
       "4  2. Most African countries have poor healthcare...   \n",
       "\n",
       "                                  Preprocessed tweet  \n",
       "0  ['still', 'learn', 'survive', 'pandemic', 'pus...  \n",
       "1  ['need', 'fight', 'covid', 'anyone', 'get', 'i...  \n",
       "2  ['trump', 'impeachment', 'distract', 'prepare'...  \n",
       "3                                 ['beat', 'corona']  \n",
       "4  ['african', 'country', 'poor', 'healthcare', '...  "
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "5c44334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f.to_csv('Preprocessed_columns.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
